{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with scikit-learn\n",
    "\n",
    "Sometimes the features you have available in your initial data have little predictive strength when used in the most straightforward way.  This might be true almost regardless of choice of model class and hyperparameters.  And yet it might also be true that there are synthetic features latent in the data that are highly predictive, but that have to be *engineered* (mechanically, rather than sample-wise modification) to produce powerful features.\n",
    "\n",
    "At the same time, a highly dimension model—whether of high dimension because of the initial data collection or because of creation of extra synthetic features—may lend itself less well to modeling techniques.  In these cases, it can be more computationally tractable, as well as more predictive, to work with a subset of all available features.\n",
    "\n",
    "This is the last of three lessons that can be thought of broadly as \"Feature Engineering.\" This lesson focuses on selecting the features of most relevance.  Within this lesson, we group in *scaling* which does not discard dimensions, but does help in providing them with relevant numeric properties for best use in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from src.setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "We have seen at several places the passing use of scaling to make data better regularized to allow models to perform best.  Scikit-learn provides several scaler classes that follow a similar API as models and other feature transformations.  Using these is mostly a simple drop-in step.\n",
    "\n",
    "To make examples simple, we generate a small amount of random data with values of \"features\" in somewhat different ranges and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 100\n",
    "test_data = np.empty((rows,4))\n",
    "test_data[:,0] = np.random.random(rows) * 2 + 1.5 \n",
    "test_data[:,1] = np.random.randn(rows)\n",
    "test_data[:,2] = np.random.randint(-50, 25, rows)/10\n",
    "test_data[:,3] = np.exp(np.random.random(rows)+1.5)\n",
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_data).describe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_data).plot(kind='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_data).plot(kind='hist', bins=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "The most commonly used scaler—as the name indicates—is `StandardScaler`. \n",
    "\n",
    "This standardizes features by removing the mean and scaling to unit variance.  It loosely assumes that the underlying data is Gaussian to start with, but mostly it is fairly robust against moderate violations of that distribution.  Calculating the mean and standard deviation is per-column (as for all scalers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled1 = StandardScaler().fit_transform(test_data)\n",
    "scaled1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled1).describe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled1).plot(kind='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled1).plot(kind='hist', bins=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RobustScaler\n",
    "\n",
    "This scales features using statistics that are robust to outliers. It removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaled2 = RobustScaler().fit_transform(test_data)\n",
    "scaled2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled2).describe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled2).plot(kind='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled2).plot(kind='hist', bins=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "Transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, by default between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaled3 = MinMaxScaler().fit_transform(test_data)\n",
    "scaled3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled3).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled3).plot(kind='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled3).plot(kind='hist', bins=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAbsScaler\n",
    "\n",
    "Scales each feature by its maximum absolute value.  This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaled4 = MaxAbsScaler().fit_transform(test_data)\n",
    "scaled4[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled4).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled4).plot(kind='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled4).plot(kind='hist', bins=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...and others\n",
    "\n",
    "A number of other scalers exists for special purposes.  An interesting one is `QuantileTransformer` that normalizes feature values by quantile (default `n_quantiles=1000`).  In effect this treats values in an ordinal way, eliminating any effect of outliers.  Output distributions other than the default `'uniform'` can be selected as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "One way to reduce features in a dataset is to throw some of them away.  In principle, features thrown away can be engineered features such as those resulting from decompositions or those discussed below that are combined or synthesized from raw features.  Scikit-learn gives you several ways to choose features to discard; or euqivalently, features to keep.\n",
    "\n",
    "There are several univariate feature selectors in scikit-learn.  `SelectKBest` is essentially the same as `SelectPercentile`, merely different in whether you indicate a number of percentage of features to keep.  Obviously, it is easy to derive one from the other based on number of features in the raw data. Just slightly different are false positive rate (`SelectFpr`), false discovery rate (`SelectFdr`), or family wise error (`SelectFwe`).  \n",
    "\n",
    "All of these let you specify a scoring function the judges the quality of the estimate using just one feature.  For regression, there are `f_regression`, `mutual_info_regression`; for classification: `chi2`, `f_classif`, `mutual_info_classif`.  You can also, in principle, use a custom function to evaluate strength of a single feature for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate\n",
    "\n",
    "Let us look at the Wisconsin cancer dataset from perspective of simply identifying the most important features (in the raw dataset).  Notice that we use the entire dataset for this purpose, not only a training portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(chi2, k=5)\n",
    "X_new = selector.fit_transform(cancer.data, cancer.target)\n",
    "cancer.data.shape, X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = pd.Series(selector.scores_, index=cancer.feature_names).sort_values(ascending=False)\n",
    "kbest.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at how models work with all 30 features versus with just the top 5 in a univariate filter.  In this example, we do not get better with this step alone (but KNN doesn't get much worse), but in some cases the score will actually improve without surplus dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "lr, knn = LinearRegression(), KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=1)\n",
    "print(\"Fit raw features:\")\n",
    "print(\" LR:\", lr.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"KNN:\", knn.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do *worse* with only five of the thirty features, but for KNN, not very much worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_new, cancer.target, random_state=1)\n",
    "print(\"Fit selected features:\")\n",
    "print(\" LR:\", lr.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\"KNN:\", knn.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Feature Selection\n",
    "\n",
    "A number of selectors are based on the chosen underlying model itself rather on univariate strengths of features.  This only works with models than expose a `.coef_` or `.feature_importances_` attribute on a trained model.  Not all algorithms allow the determination of those values.\n",
    "\n",
    "We look here at recursive feature elimination which is based on a model, and arrives at the most important features by eliminating the least important, then refitting based on those that remain, and doing so repeatedly until only the number of features requested remain.  The selector `SelectFromModel` works in a similar way, but recursive selection will generally be better quality (albeit **much** slower since it involves repeated refitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive feature elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "lr = LinearRegression()\n",
    "rfe = RFE(estimator=lr, n_features_to_select=5, step=1) \n",
    "rfe.fit(cancer.data, cancer.target)\n",
    "\n",
    "pd.Series(rfe.ranking_, index=cancer.feature_names).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative importance of features is similar, but somewhat different under recursive feature elimination than under K-best selection.  Given the nature of linear models, we mostly expect univariate elimination to behave similarly to recursive elimination based on linear regression.  Something like a decision tree (which provides `.feature_importances_`) might have a more different ranking of selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go farther here and use `RFECV` to recursive eliminate features, cross-validate the model at each stage, and keep a record of the relationship between model score and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=7, random_state=1)\n",
    "rfecv = RFECV(estimator=rf)\n",
    "rfecv.fit(cancer.data, cancer.target)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.title(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Selectors\n",
    "\n",
    "In the right circumstances, it can make a **huge** difference to create synthetic features and then to select among them in the best way.  Let's go all out in trying to create a genuinely *good* model for this cancer data.  That often involves everything we have seen: polynomial expansion, scaling, and feature selection.\n",
    "\n",
    "Given relatively high initial accuracy, it is more illustrative to look at how often a model is *wrong* than its accuracy to highlight differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(cancer.data)\n",
    "rfc = RandomForestClassifier(max_depth=7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=42)\n",
    "\n",
    "acc = rfc.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Raw features, error rate: {(1-acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the engineered features (good practice, not directly helpful here)\n",
    "# NOTE: we use MinMaxScaler here rather than StandardScaler,\n",
    "#       because chi^2 test requires positive features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_poly_scaled = MinMaxScaler().fit_transform(X_poly)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly_scaled, cancer.target, random_state=42)\n",
    "\n",
    "acc = rfc.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Scaled polynomial features, error rate: {(1-acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the best 100 features in a univariate variate way rather than swamp the model with all 496 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(chi2, k=100)\n",
    "X_new = selector.fit_transform(X_poly_scaled, cancer.target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_new, cancer.target, random_state=42)\n",
    "\n",
    "acc = rfc.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Univariate best 100 poly, error rate: {(1-acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we spend a while longer, we can use *Recursive Features Elimination with Cross-Validation* on all 496 of our polynomial features.  This involves retraining a Random Forest composed of 7 Decision Trees 496 separate times.  So it is some computational work.  \n",
    "\n",
    "The `n_jobs=-1` parameter says to use all the CPU cores on the current machine (on the machine I usually teach on, that is 8 cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly_scaled, cancer.target, random_state=42)\n",
    "\n",
    "rfecv = RFECV(estimator=rfc, n_jobs=-1)\n",
    "acc = rfecv.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"RFECV best {rfecv.n_features_} of {X_poly_scaled.shape[1]}\", \n",
    "      f\"error rate: {(1-acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result here is that there are 37 synthetic features—not so different in number from the original 30 dimensions—that are not only vastly better than the original features, but in facts have half the error rate of choosing 100 of the polynomial features in a univariate way.\n",
    "\n",
    "For what it is worth, I tried `k=50` and `k=100` is better, but I did not try every univariate k.  But remember that the order of selection of univariate features is fixed, so we would definitely have to go to more than 100 features to get these 37 \"Goldilocks\" features (and going too high will weaken the model because of the *curse of dimensionality*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a passing note, I also tried using a degree-3 polynomial to create thousands of synthetic features, then after a very long run time (hours), produced something worse than the degree-2 example above.  The problem was that RFECV still selected over a thousand features in this example. Possibly other mechanisms to more aggressively winnow degree-3 features could eek out an incremental gain, but also likely is that degree-2 is best suited for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next lesson\n",
    "\n",
    "**Pipelines**: In this lesson we dealt with quite a lot of what we might call *data engineering*. The need for feature selection, feature engineering, and scaling, goes beyond the base fact that \"data always comes in messy.\"  Certainly the need for data cleaning is important, and has probably not been addressed to match that importance in this course.  But even data that is spotless for an data integrity and anomaly perspective (a lofty goal) might still need to be massaged to allow models to succeed in their magic.\n",
    "\n",
    "The next lesson on pipelines will be fairly short, but very important.  We have seen a variety of ways we can manipulate data—and also ways we can tune models with hyperparameters—but we have done each of these steps one-by-one, in explicit code with extra intermediate steps lying around along the way.  Pipelines allow us to package all these steps together in one resusable API.\n",
    "\n",
    "<a href=\"Pipelines.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
